{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "13nmWHpfj9uvyW7ADo6Yh_FoCRIJ3lS-j",
      "authorship_tag": "ABX9TyMDkSKvIN0Nbw/1DafaXOj8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nisha-sah/Concepts-and-Technologies-of-AI/blob/main/Woerkshop5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTCn-m9RRj5x",
        "outputId": "2706a5c1-fd87-42a4-b279-cbb54e9494b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceed Further\n",
            "Cost function output: 0.0\n",
            "Proceed Further\n",
            "Cost function output: 0.0\n"
          ]
        }
      ],
      "source": [
        "#To - Do - 4:\n",
        "import numpy as np\n",
        "\n",
        "def cost_function(X, Y, W):\n",
        "    \"\"\"\n",
        "    This function finds the Mean Square Error.\n",
        "    \"\"\"\n",
        "    y_pred = X.dot(W) #length\n",
        "    m = len(y_pred)\n",
        "\n",
        "\n",
        "    cost = np.sum((y_pred - Y) ** 2) / (2 * m)#formula\n",
        "\n",
        "    return cost\n",
        "\n",
        "# Test data\n",
        "X_test = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "Y_test = np.array([3, 7, 11])\n",
        "W_test = np.array([1, 1])\n",
        "\n",
        "# Compute cost\n",
        "cost = cost_function(X_test, Y_test, W_test)\n",
        "\n",
        "if cost == 0:\n",
        "    print(\"Proceed Further\")\n",
        "else:\n",
        "    print(\"something went wrong: Reimplement a cost function\")\n",
        "\n",
        "print(\"Cost function output:\", cost)\n",
        "\n",
        "\n",
        "# Test data\n",
        "X_test = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "Y_test = np.array([3, 7, 11])\n",
        "W_test = np.array([1, 1])\n",
        "\n",
        "# Compute cost\n",
        "cost = cost_function(X_test, Y_test, W_test)\n",
        "\n",
        "if cost == 0:\n",
        "    print(\"Proceed Further\")\n",
        "else:\n",
        "    print(\"something went wrong: Reimplement a cost function\")\n",
        "print(\"Cost function output:\", cost)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize linear regression parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    cost_history = [0] * iterations\n",
        "    m = len(Y)\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "\n",
        "        # 1. Prediction (FIXED)\n",
        "        Y_pred = np.dot(X, W)\n",
        "\n",
        "        # 2. Error\n",
        "        loss = Y_pred - Y\n",
        "\n",
        "        # 3. Gradient\n",
        "        dw = (1 / m) * np.dot(X.T, loss)\n",
        "\n",
        "        # 4. Update weights (FIXED)\n",
        "        W = W - alpha * dw\n",
        "\n",
        "        # 5. Cost\n",
        "        cost = (1 / (2 * m)) * np.sum(loss ** 2)\n",
        "        cost_history[iteration] = cost\n",
        "\n",
        "    return W, cost_history\n",
        "import numpy as np\n",
        "\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize linear regression parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    cost_history = [0] * iterations\n",
        "    m = len(Y)\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "\n",
        "        # 1. Prediction (FIXED)\n",
        "        Y_pred = np.dot(X, W)\n",
        "\n",
        "        # 2. Error\n",
        "        loss = Y_pred - Y\n",
        "\n",
        "        # 3. Gradient\n",
        "        dw = (1 / m) * np.dot(X.T, loss)\n",
        "\n",
        "        # 4. Update weights (FIXED)\n",
        "        W = W - alpha * dw\n",
        "\n",
        "        # 5. Cost\n",
        "        cost = (1 / (2 * m)) * np.sum(loss ** 2)\n",
        "        cost_history[iteration] = cost\n",
        "\n",
        "    return W, cost_history\n",
        "\n",
        "\n",
        "\n",
        "#test\n",
        "np.random.seed(0)\n",
        "\n",
        "X = np.random.rand(100, 3)\n",
        "Y = np.random.rand(100)\n",
        "W = np.random.rand(3)\n",
        "\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "\n",
        "final_params, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
        "\n",
        "print(\"Final Parameters:\", final_params)\n",
        "print(\"Final Cost:\", cost_history[-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXNq4EHOZK48",
        "outputId": "71c80eb2-6cd7-4373-a059-b839016b9aa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Parameters: [0.20551667 0.54295081 0.10388027]\n",
            "Final Cost: 0.05435809373901896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# RMSE Function\n",
        "\n",
        "def rmse(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This Function calculates the Root Mean Square Error.\n",
        "    \"\"\"\n",
        "    return np.sqrt(np.mean((Y - Y_pred) ** 2))\n",
        "\n",
        "\n",
        "\n",
        "# R-Squared Function\n",
        "\n",
        "def r2(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This Function calculates the R Squared Error.\n",
        "    \"\"\"\n",
        "    mean_y = np.mean(Y)\n",
        "    ss_tot = np.sum((Y - mean_y) ** 2)\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "    return 1 - (ss_res / ss_tot)\n",
        "\n",
        "\n",
        "# Gradient Descent Function\n",
        "\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Performs Gradient Descent to optimize Linear Regression weights.\n",
        "    \"\"\"\n",
        "\n",
        "    m = len(Y)\n",
        "    cost_history = []\n",
        "\n",
        "    for _ in range(iterations):\n",
        "\n",
        "        # Step 1: Prediction\n",
        "        Y_pred = np.dot(X, W)\n",
        "\n",
        "        # Step 2: Error\n",
        "        error = Y_pred - Y\n",
        "\n",
        "        # Step 3: Cost (Mean Squared Error)\n",
        "        cost = (1 / (2 * m)) * np.sum(error ** 2)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "        # Step 4: Gradient\n",
        "        gradient = (1 / m) * np.dot(X.T, error)\n",
        "\n",
        "        # Step 5: Update weights\n",
        "        W = W - alpha * gradient\n",
        "\n",
        "    return W, cost_history\n",
        "\n",
        "\n",
        "# Main Function\n",
        "\n",
        "def main():\n",
        "\n",
        "    # Step 1: Load the dataset\n",
        "    data = pd.read_csv(\n",
        "        '/content/drive/MyDrive/Week - 2 - Describing the World with Data/Dataset/student.csv'\n",
        "    )\n",
        "\n",
        "    # Step 2: Split features and target\n",
        "    X = data[['Math', 'Reading']].values\n",
        "    Y = data['Writing'].values\n",
        "\n",
        "    # Step 3: Train-test split\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "        X, Y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Step 4: Initialize parameters\n",
        "    W = np.zeros(X_train.shape[1])\n",
        "    alpha = 0.00001\n",
        "    iterations = 1000\n",
        "\n",
        "    # Step 5: Gradient Descent\n",
        "    W_optimal, cost_history = gradient_descent(\n",
        "        X_train, Y_train, W, alpha, iterations\n",
        "    )\n",
        "\n",
        "    # Step 6: Predictions\n",
        "    Y_pred = np.dot(X_test, W_optimal)\n",
        "\n",
        "    # Step 7: Evaluation\n",
        "    model_rmse = rmse(Y_test, Y_pred)\n",
        "    model_r2 = r2(Y_test, Y_pred)\n",
        "\n",
        "    # Step 8: Results\n",
        "    print(\"Final Weights:\", W_optimal)\n",
        "    print(\"Cost History (First 10):\", cost_history[:10])\n",
        "    print(\"RMSE:\", model_rmse)\n",
        "    print(\"R-Squared:\", model_r2)\n",
        "\n",
        "\n",
        "\n",
        "# Execute Program\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "i3vBV54Aea6o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de5a9fc8-1313-4dee-b03b-28ce59f8fb85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [0.34811659 0.64614558]\n",
            "Cost History (First 10): [np.float64(2471.69875), np.float64(2013.165570783755), np.float64(1640.286832599692), np.float64(1337.0619994901588), np.float64(1090.4794892850578), np.float64(889.9583270083234), np.float64(726.8940993009545), np.float64(594.2897260808594), np.float64(486.4552052951635), np.float64(398.7634463599484)]\n",
            "RMSE: 5.2798239764188635\n",
            "R-Squared: 0.8886354462786421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#To do\n",
        "# 1\n",
        "#Final Weights: [0.34811659 0.64614558]\n",
        "#Cost History (First 10): [np.float64(2471.69875), np.float64(2013.165570783755), np.float64(1640.286832599692), np.float64(1337.0619994901588), np.float64(1090.4794892850578), np.float64(889.9583270083234), np.float64(726.8940993009545), np.float64(594.2897260808594), np.float64(486.4552052951635), np.float64(398.7634463599484)]#\n",
        "\n",
        "#RMSE: 5.2798239764188635\n",
        "#R-Squared: 0.8886354462786421\n",
        "\n",
        "1.Interpretation:\n",
        "\n",
        "The training process reduce the cost history .\n",
        "Validation/test error is not much higher than training error\n",
        " (based on RMSE and R²), which indicates the model is performing well. this will be closed show the machine is working perfectly\n",
        "RMSE ≈ 5.28 → small error, but not zero\n",
        "\n",
        "R² ≈ 0.8886 → very good, but not 1\n",
        "\n",
        "The model is performing very well, not overfitting or underfitting.\n",
        "\n",
        "model is performing well and reliable, but not perfect (RMSE isn't 0, R² isn't 1).\n",
        "\n",
        "\n",
        "2.α = 0.01\n",
        "Learning Rate              Observation                                Result\n",
        "α = 0.1(bigger)           Cost changes rapidly                        Faster learning but may be unstable\n",
        "\n",
        "α = 0.001(smaller)       Cost decreases slowly                        Stable training but takes longer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "75jRx2nNB9-A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}